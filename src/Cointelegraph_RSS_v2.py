import feedparser
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
from datetime import datetime, timedelta
import time
import json

# üïµÔ∏è Scrape short summary and full content via Playwright
def get_cointelegraph_post_and_content(url):
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(url, timeout=60000)
            page.wait_for_timeout(3000)

            html = page.content()
            browser.close()

            soup = BeautifulSoup(html, "html.parser")

            # Get short post from <meta name="description">
            meta_desc = soup.find("meta", {"name": "description"})
            post = meta_desc["content"] if meta_desc and meta_desc.has_attr("content") else ""

            # Get full content from article paragraphs
            content_container = soup.select_one("div.post-content.relative")
            paragraphs = content_container.find_all("p") if content_container else []
            url_content = "\n".join(p.get_text(strip=True) for p in paragraphs[:8]) if paragraphs else "‚ö†Ô∏è No <p> tags found"

            return post, url_content

    except Exception as e:
        print(f"‚ùå Error fetching article content from {url} ‚Üí {e}")
        return "", ""

# üì° Fetch articles from RSS and filter last 24h
def fetch_cointelegraph_last_24h():
    feed_url = "https://cointelegraph.com/rss"
    feed = feedparser.parse(feed_url)
    articles = []

    now = datetime.utcnow()
    cutoff = now - timedelta(days=1)

    for entry in feed.entries:
        published_parsed = entry.get("published_parsed")
        if not published_parsed:
            continue

        published_dt = datetime.fromtimestamp(time.mktime(published_parsed))
        if published_dt < cutoff:
            continue

        title = entry.get("title", "").strip()
        url = entry.get("link", "").strip()

        print(f"üïí [{published_dt}] Scraping: {title}")
        post, url_content = get_cointelegraph_post_and_content(url)

        articles.append({
            "title": title,
            "post": post,
            "url": url,
            "views": None,
            "reposts": None,
            "url_content": url_content,
            "source": "Cointelegraph",
            "published": published_dt.isoformat()
        })

    return articles

# ‚úÖ Run + Save
if __name__ == "__main__":
    articles = fetch_cointelegraph_last_24h()

    for i, a in enumerate(articles, 1):
        print(f"[{i}] {a['title']}")
        print(f"üïí Published: {a['published']}")
        print(f"üì∞ Post: {a['post'][:100]}...")
        print(f"üìÑ Content: {a['url_content'][:300]}...")
        print(f"üîó {a['url']}")
        print("-" * 60)

    with open("cointelegraph_24h_articles.json", "w", encoding="utf-8") as f:
        json.dump(articles, f, ensure_ascii=False, indent=2)

    print(f"\n‚úÖ Saved {len(articles)} articles to cointelegraph_24h_articles.json")
